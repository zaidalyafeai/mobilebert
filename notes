* do evaluation on v3-8 
* num_train_steps (training done after distillation)
num_distill_steps (disillation before) 
* we can run v3-128 on preemptible with batch size 2048
* discrepency between the num_attention_heads between arabert and mobilebert cause an error 
* reduced the mobilebert size to 26 M by reducing the num_att_heads=4 and vocab_size
  decreasing vocabsize in mobilelebert doesn't cause error [this is because training from arabert didn't initialize from ckpt]
* masked lm accuracy  on poems (10K lines) around 60% after 10K
and 80% after 15K and then 90% after 20K. (training from scratch)
* masked lm accuracy is 94 % after pretraining ibert for 5K then distill 10K on mobilebert
It takes around 2 hours to run 20K iterations using the 26M mobilebert
* ibert's 1000 steps takes around 10 mins on v3-8 . If my calcs are correct then it should take around 16 hrs to run 100K iterations. (256 batch size).
